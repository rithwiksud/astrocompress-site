\section{Introduction}
\label{sec:intro}
Machine learning is having an increasingly large impact on natural sciences \citep{RevModPhys.91.045002,2023Natur.620...47W}. One of the primary hurdles of data-driven scientific discovery is bandwidth bottlenecks in data collection and transmission, particularly if the data is collected autonomously in high-throughput scientific instruments like telescopes or biological sequencing devices. Pairing this with the precision demands of many scientific domains, there is a significant interest for improved data compression methods across scientific domains.


A compression algorithm, or \emph{codec}, is comprised of a pair of algorithms that can encode data into a smaller size, and then decode back to the original or near-original data. Neural compression algorithms \citep{yang2023introduction} have recently surpassed traditional codecs on image \citep{balle2017end, yang2020improving, he2022elic} and video compression \citep{lu2019dvc, agustsson2020scale, yang2021hierarchical, mentzer2022vct}. 
% Although standardization efforts are ongoing, the real-world deployment of neural compression is hindered by high computational resource requirements \citep{minnen_current_2021}.
% the complexities of modifying existing systems and data pipelines.
% In contrast, neural compression has the potential for even greater and faster impact across various scientific disciplines, such as astrophysics, biology, and climatology. The data collected in these fields exhibit statistical patterns and signal-to-noise distributions distinct from RGB-camera images common to large image corpora. Traditional image codecs have been meticulously hand-crafted over the past five decades, but replicating this process for the diverse and emerging data formats in natural science domains is impractical. Neural compression stands out as the preferred approach here, enabling end-to-end learning from data and significantly accelerating development while exploiting patterns that may be imperceptible to humans.
% While most existing work focuses on visual media applications, where 
Compared to these visual media applications, where fast decoding is crucial, \citep{minnen_current_2021}, scientific domains often prioritize compression performance and/or encoding speed. This is because data analysis can be done on powerful supercomputers over days or weeks, but the instruments' data collection rates are extremely high and data must be transmitted rapidly. Science can more readily and substantially benefit from neural compressionâ€”which generally achieves high compression rates at the expense of time \citep{yang2023computationally}.
% While their high computational complexity is an issue in their widespread adoption in visual media applications \cite{minnen_current_2021}, its potential impact across scientific disciplines like astrophysics, biology, and climatology is substantial. 
% These fields


Scientific data exhibit unique statistical patterns and signal-to-noise distributions, potentially making traditional handcrafted codecs less suitable. Neural compression, being learned end-to-end from the data, can automatically exploit redundancies occurring in the data and accelerate the development of practical codecs for custom applications.
% offers a promising solution by accelerating development and uncovering subtle patterns imperceptible to human observers.

%Introduce AstroCompress.
%Existing datasets lack xyz. 
%Contributions list.

\paragraph{We are at the cusp of a scientific data explosion.} Current imaging efforts mapping the mouse hippocampus \citep{neuralmapping2024} are estimated to contain 25 petabytes of imaging data. Human genomics data is expected to generate 2--10 exabytes of data over the next decade \citep{nih2024}. The Square Kilometer Array (SKA), a ground-based telescope, is expected to collect 62 exabytes per year \citep{2018Galax...6..120F}, where improvements in lossless compression could significantly reduce storage costs.

In 2027, NASA will launch the Nancy Grace Roman Space Telescope, or "Roman." A recent audit \citep{NASA_2024} emphasizes that the single greatest concern for Roman is data transmission issues due to an "unprecedented" data scale and unprepared downlink networks that lack the necessary bandwidth. Space telescopes suffer a unique problem: due to limited bandwidth and onboard storage, any data that cannot be transmitted must be deleted and is thus permanently lost. Given Roman's estimated cost of \$$4.3$ billion, a mere $1\%$ improvement in lossless compression for Roman thus imputes a value of \$$43$ million on the additional data gathered. This will be the first space telescope with the ability to run modern codecs, as the recently launched James Webb Space Telescope (JWST) was planned more than 20 years ago and has highly limited compute onboard \citep{2023PASP..135f8001G}.

Astrophysics presents novel challenges and opportunities for learned compression. Astronomical imaging captures the locations, spatial extent, temporal changes, and colors of celestial objects and events. Despite variations in detector and instrumentation physics across the electromagnetic spectrum, much of the raw data is stored as 2D digital arrays, mapping intensity at pixel locations $(x, y)$ to a projection of the flux of objects on sky. Repeated integrations enhance depth (higher signal-to-noise) through post-processing co-addition and can be used to measure time variations in celestial events. Color information is obtained from co-spatial observations using different instruments or filters with selective wavelength sensitivity.
%In particular, the domain of astrophysics poses unique challenges and opportunities for learned compression. Astronomical imaging observations capture the locations, spatial extent, temporal changes, and color of celestial objects and events. While detector and instrumentation physics varies widely across the electromagnetic spectrum, in practice, much of the raw data acquired is captured and stored in 2-dimensional (2-D) digital arrays, with pixel locations $(x, y)$ mapping to a projection of a portion of the sky. Repeated integrations are used to achieve greater depth (ie., higher signal-to-noise) via post-processing co-addition and/or measure time variations of the objects of interest. Color information can be obtained by co-spatial observations with different instrument or filter configurations that provide selective sensitivity at different wavelengths.


As detector sizes grow and costs per pixel decline, larger optical and infrared arrays are being deployed. The Rubin Observatory, now in commissioning, has the largest optical camera ever built, with 189 CCDs comprising 3.2 billion pixels \citep{2010SPIE.7735E..0JK}. It can generate 20--30 TB of raw imaging data per night with 5-second integrations. The main imaging camera of JWST features ten $2048\times2048$\,pix$^{2}$ detectors \citep{2023PASP..135f8001G}, compared to the single $256\times256$\,pix$^{2}$ array of the Hubble Space Telescope \citep{1998ApJ...492L..95T}. Unlike CCDs and CMOS detectors, which are read only once per exposure, IR arrays (e.g. that of JWST) are continuously sampled during integration, producing 3D data cubes with arbitrarily large temporal samples. Optical and IR instruments convert photon signals captured by pixelated semiconductor detectors into digital values using analog-to-digital (A/D) converters, resulting in 16-bit depth images. Each pixel value includes flux from astronomical sources (which we refer to as ``sources'' in the rest of the paper), sky background, A/D-introduced noise, and ``dark current'' arising from the non-zero detector temperature.
%As the commercially available detector sizes grow while costs per pixel continue to decline, optical and infrared sensitive arrays are being deployed in ever-larger formats. For example, the focal plane of the LSST/Rubin facility, now in commissioning phase, is the largest optical camera ever built with 189 CCDs comprising 3.2\,billion pixels \citep{2010SPIE.7735E..0JK}. With integrations as short as 5\,sec, it could theoretically generate $\sim$20--30 TB of raw imaging per night. At infrared wavelengths, the main imager for the newly commissioned James Webb Space Telescope (JWST) contains ten $2048\times2048$\,pix$^2$ detectors \citep{2023PASP..135f8001G} whereas the equivalent for the Hubble Space Telescope (HST) was comprised of a single 256$\times$256\,pix$^2$ array \citep{1998ApJ...492L..95T}.  Whereas CCDs and CMOS detectors are read destructively (ie., once per exposure), IR arrays are continuously sampled (``up the ramp'') during integration and so a single exposure can produce 3-D data cubes with the number of samples $s$ being arbitrarily large. Optical and IR instruments translate signals of photons captured with pixelated semiconductor detectors to digital values using analog--to--digital (A/D) converters, usually resulting in raw unsigned integer images with 16-bit depths (ie., \texttt{uint16}). The value of a given pixel will contain the additive result of the cumulative flux from astronomical sources at that sky location, sky background (e.g., airglow, moonlight), A/D-introduced noise, and ``dark current'' induced by the microphysics of the non-zero temperature of the detector.

The need to transmit large amounts of data efficiently and losslessly is a critical challenge for premier astronomical facilities, which often operate in remote locations to optimize observations. 
%Ground-based observatories are placed in dry, high-altitude regions like Antarctica, Hawaii, and the Chilean Andes to avoid light pollution and atmospheric blurring. Space-based facilities benefit from being far from Earth and moon-shine and in thermally stable orbits. 
This remoteness complicates data transmission to distant computational and archival centers. For ground-based facilities, the inability to transmit raw data as quickly as it is obtained (e.g., via wired internet) means hard copies of the data must be (periodically) moved physically, degrading time-sensitive science \citep{Bezerra}. Space-based facilities risk losing data if it cannot be transmitted with high-enough bandwidth.
%One common characteristic of the world's premier astronomical facilities is that they operate in  some of the most remote sites possible. Ground-based observatories need to be placed far from light polluting populations, in dry regions without wind, and at high altitudes to reduce the effects of atmospheric blurring. Antartica, Hawaii, and the high-desert of the Chilean Andes mountains are thus considered the best observing sites. Similarly, space-borne facilities benefit by being far Earth and moon-shine and in thermally stable orbits. This remoteness correlates strongly with the difficulty of transmitting data from observatories to computational and archival data centers, near where astronomers begin to make use of data for conducting scientific inquiry. For ground-based facilities, the inability to transmit raw data as quickly as it is obtained via wired internet or radio satellite communication, means hard copies of the data must be (periodically) moved physically, leading to the degradation of science (especially time-domain science)\footnote{To accomodate the LSST/Rubin data rate, a special 100 Mbps fiber network has to be built from Chile to Miami at great expense \citep{Bezerra}.}. For space-based facilities, the inability to sustainably transmit raw data to the ground as quickly as it is effectively obtained means it is lost forever.
%
These constraints are practical and evident in current space missions. The Kepler mission \citep{2010Sci...327..977B} produced $\sim$190 MB of raw data from 42 CCDs every ~6 seconds but had to coadd 10--250 exposures and transmit only selected pixels around 200,000 preselected stars due to bandwidth and storage limits \citep{10.1117/12.897767}. The TESS satellite \citep{2015JATIS...1a4003R} combines 2-second integrations from 4 CCDs into frames of 60 seconds or 30 minutes and transmits only the 10$\times$10 pixels around pre-designated objects and full-frame 30-minute coadds. JWST enforces strict data rate limits during observations to stay within deep space downlink constraints, affecting data collection and storage \citep{jwst}.
%These constraints are not theoretical, but are manifest in practice. The NASA/Kepler mission \citep{2010Sci...327..977B}, designed to find extrasolar planets using the time-domain transit technique, produced $\sim190$ MB of raw \texttt{utin14} data from 42 2200$\times$1024 pix$^2$ CCDs every $\sim6$\,sec but, due to bandwidth and on-board storage limitations was required to (lossly) coadd 10--250 exposures and telemeter only a finite set of subraster ``pixels of interest'' (themselves quantized with a lossly algorithm) around 200k preselected stars of interest \citep{10.1117/12.897767}. The NASA/TESS exoplanet satellite \citep{2015JATIS...1a4003R}, (lossly) combines 2\,sec integrations from 4 2048$\times$2048 CCDs into single frames of short (60\,sec; 30 raw images) and long (30\,min; 900 raw images) effective duration. Only those 10$\times$10 pixels of interest around pre-designated objects are assembled from the short co-adds and are sent to ground along with the full-frame 30\,min coadds. To stay within the extreme downlink constraints from deep space, NASA/JWST maintains strict limitations on the data rate that an observer can generate during their scheduled observations, directly affecting the way in which data is taken and the amount that is saved \citep{jwst}.
%
% \textcolor{red}{Since data is a core product of modern observatories, any limitations in transmission bandwidth fundamentally constrain the science that can be done at our facilities. Modern instruments are expensive to develop and thus any improvement made in lossless compression can scale to billions of dollars worth of additional scientific data made available to the community. As such, losslessly compressing raw imaging data before transmission is highly desirable, balancing better compression ratios against compute and hardware costs, especially for space-based facilities.} 
Bottlenecks in data transmission directly impact the scientific potential of modern observatories. Given the high cost of developing and maintaining modern observatory instruments, improved data compression can significantly increase the value of data available to researchers. Lossless compression of raw imaging data before transmission is thus highly desirable, especially for space-based facilities, balancing improved compression ratios against computational and hardware costs.
%While compression also reduces file sizes in data centers and speeds up data transfer, all use cases benefit from improved compression ratios.



%Transmission bandwidth limitations thus serve as a fundamental constraint on the amount of data that can be obtained and ultimately act as a limiting factor in the design and costing of any new facility.  Compression of raw imaging data before transmission from the observatory is thus highly desirable.  Better compression ratios ultimately must be traded-off against additional compute costs (with special emphasis on energy requirements for space-based facilities) and additional hardware costs for both encoding on-site and decoding remotely. While there are other important use cases for compression, namely to reduce file sizes at rest (e.g., in data centers) and to improve the speed to move data across the internet (e.g., from the data center to a scientist's own compute cluster), all three use cases can benefit from better compression ratios.

This benchmark serves multiple purposes. Firstly, we release several large, unlabeled astronomy datasets spanning the range of imaging found across modern astronomy. We note that astronomy has several other modes of data collection, such as radio interferometry and spectroscopy, both of which are out of scope for our work. Our goal is to foster a machine-learning community focused on astrophysical data compression, aiming for improved compression methods that can eventually be practically deployed on space telescopes. Secondly, we benchmark several neural lossless compression algorithms on the data, demonstrating that neural compression has significant potential to outperform classical methods. We hope that our initial results will encourage further research (cf.~\citealt{2022NatRP...4..413T}), leading to even better outcomes and a deployable algorithm. 
% Finally, we hope this dataset inspires new advancements in neural \emph{lossless} compression, which has been less impactful in common contexts like internet media.
Future astronomy codecs can also be developed and benchmarked on this dataset for \emph{lossy} compression, where we anticipate significant progress and improvements. 

In sum, our main contributions to the datasets and benchmarks track are as follows:%\vspace{-0.57cm}
\begin{itemize}
\item A large ($\sim$320\,GB), novel dataset captures a broad range of real astrophysical imaging data, carefully separated into train and test sets, with easy access via HuggingFace \texttt{datasets}.
\item An extensive comparison of  lossless classical and neural compression methods on this data, the first publication to our knowledge where neural compression has been systematically studied on astronomical imaging.
\item Various qualitative analyses that further our understanding of the bit allocation in astronomical images and inform potential exploration for future \textit{lossy} compression codec designs.
\end{itemize}


%  Petabyte-scale data collection is rapidly becoming the backbone of modern astronomy. As a leading example, the Square Kilometer Array (to be completed in 2027) has been estimated to collect 1 exabyte per day of raw data. On the other hand, space-based instruments such as the James Webb Space Telescope are severely limited in bandwidth due to the noisy channel of space as well as the requirement for line-of-sight contact with Earth, which only allows downlink to happen for 8 hours per day. Consequently, JWST can transmit up to 60 GB per day.