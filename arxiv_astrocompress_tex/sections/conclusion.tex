\section{Conclusion}
\label{sec:conclusion}


%the adoption of JPEG-XL for telescopes immediately

AstroCompress aims to incentivize the development of astronomy-tuned neural codecs for eventual real-world deployment, by providing datasets that are representative of real use cases. While this work focuses on lossless neural compression, many other machine learning tasks are intimately related. Compression is ``bijectively'' linked with likelihood estimation by Shannon's source coding theorem \citep{shannon1948mathematical}. We suggest the use of our dataset in other machine learning for astronomy contexts, such as self-supervised learning for foundation models, semantic search and anomaly detection. Improved lossless and near-lossless neural codecs explored through AstroCompress will likely transfer to other kinds of high resolution, high bit-depth imagery, such as satellite imagery, radio astronomy and biological imaging (as mentioned in Section \ref{sec:intro}).

%, image translation between wavelength bands, or learning the distribution of time-series imagery (the variation within which is complex, but mostly due to myriad noise sources and not astrophysical objects).\textbf{Limitations}

%The exploration in AstroCompress will likely transfer to other kinds of high-resolution, %high-bit depth imagery, such as radio astronomy, satellite and biological imaging. 
%The Square Kilometer Array is expected to collect 62 exabytes per year %\citep{2018Galax...6..120F} and improvement in lossless compression could translate to a %meaningful reduction in storage costs. {Current efforts} \citep{neuralmapping2024} in %mapping the mouse hippocampus are estimated to contain 25 petabytes of imaging data. %Extreme demand for \textit{lossless} data compression will soon extend to other data types %as well, such as {human genomics data} \citep{nih2024}, which is expected to generate 2-%-10 exabytes of data over the next decade.

\subsection{Limitations}

Limitations of our dataset include a lack of spectroscopic, radio astronomy or floating-point data. We leave much work to be done on neural methods that can efficiently compress 3D data cubes. Finally, to reduce computation, our compression results for neural compression methods are obtained from evaluating likelihoods under the models without entropy coding. Our relatively inefficient entropy coding implementation shows a negligible overhead compared to bit-rate estimates, and we leave  more efficient implementations of the neural compression algorithms to future work.



%In addition, we believe that the significant time delay in between our SDSS-3DT image exposures may not be representative of other optical telescopes that may do back-to-back exposures of the same region. Such a dataset may bring forth significant compression gains, the way we have shown for back-to-back infrared imaging via JWST frame differencing.

%Our goal is to jumpstart a wave of research in astronomy-specific and science-specific lossless neural compression by releasing a carefully curated, accessible dataset paired with easily referenceable benchmarks. We hope this will be the beginning of a research field leading to new models that set a significantly improved standard for scientific data compression, even if our benchmarks have not yet done so.

%Additionally, the three neural compression methods we examined, although established in the literature, likely do not represent the state-of-the-art performance in neural lossless compression. However, this was a conscious decision. While recent deep generative models such as diffusion models \citep{kingma2021variational} can in principle be used with an entropy coder for lossless compression, the computation requirement is likely to be impractically high for them to be deployable, in addition to engineering challenges of compression with deep latent variable models, such as errors from floating point arithmetic \citep{balle2018integer} and the initial-bits problem associated with bits-back coding \citep{kingma2019bit}. More research is necessary in this field, but beyond the scope of this current paper.






