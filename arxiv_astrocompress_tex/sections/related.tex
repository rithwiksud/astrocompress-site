\section{Background and Related Work}
\label{sec:relatex}
Compression is widely used in astronomy to transmit raw data from satellites, to store smaller files in archives, and to speed the movement of files across networks.  The consultative Committee for Space Data Systems \citep{ccsds2024} periodically reports on recommendations for methods---all variants of JPEG-LS and JPEG-2000 (see below), though not yet JPEG-XL---to compress images and data cubes as they are transmitted in packets from satellites to ground stations. Specialized commercially available space-hardened hardware modules that implement these standards are in use both by NASA and the European Space Agency (ESA). Once on the ground, images are usually held and transmitted by major archives with bzip2, Hcompress, or Rice compression (cf.~\citealt{pence2009lossless} and \S \ref{sec:methods}). Compressed image storage (and manipulation codes; \citealt{2010ascl.soft10002S}) are standardized in the file formats, like FITS \citep{2010A&A...524A..42P} and HDF5, commonly in use by astronomers. Many have studied and proposed refinements of these methods both for lossless~\citep{villafranca2013prediction, pata2015astronomical, thomas2015learning, maireles2023efficient,mandeel2021comparative} and lossy~\citep{maireles2022analysis} compression. Notably, all of these works rely on manually designed codecs using 
relatively simple probability models and classical transforms from signal processing.
% classical probability distributions and transforms.

% In contrast to traditional hand-designed compression algorithms developed for general purposes, the promise of neural compression lies in learning compression codecs end-to-end from specialized datasets~\citep{yang2023introduction}. By understanding the data distribution, these algorithms can optimally assign code vectors to binary bitstrings, offering significant improvements in both lossy and lossless compression performance (within model limitations). 


In contrast to traditional hand-designed codecs, neural compression algorithms based on deep generative models \citep{yang2023introduction} can be optimized end-to-end on the target data of interest, and has demonstrated significantly improved compression performance.
Lossless compression involves estimating a probability model of the data, and neural lossless methods are typically built on
% Neural compression can be categorized into lossy and lossless methods. Lossy compression typically focuses on optimizing a trade-off between bit-rate and reconstruction error (distortion), %  a rate-distortion loss function, balancing bitrate and distortion, 
% based on a (variational) autoencoder model and extensions~\citep{theis2017lossy, balle2017end, balle2018variational, mentzer2020high, yang2023lossy}. For lossless compression, the primary goal is to approximate the density of discrete data. Models used in this approach include 
discrete normalizing flows~\citep{hoogeboom2019integer}, diffusion models~\citep{kingma2021variational}, VAEs~\citep{townsend2019practical,mentzer2019practical}, and probabilistic circuits~\citep{liu2022lossless}. 
Unlike lossy compression which requires evaluating the (lossy) reconstruction quality, lossless compression is primarily concerned with the bit-rate (the lower the better) or compression ratio (the higher the better); however building a neural lossless codec with high compression ratio and low computation complexity remains a challenge.
% Although lossless compression is more straightforward as it does not involve trade-offs like distortion vs. realism~\citep{blau2018perception} or constructing task-specific losses~\citep{dubois2021lossy, matsubara2022supervised}, developing a practical codec with very high compression ratio remains a challenge. 
% an effective lossless compressor remains a challenging generative modeling task, with no definitive leading model.

A small but growing community has been testing neural compression methods for data from specialized scientific domains. \citet{hayne2021using} published a study on neural compression for image-like turbulence and climate data sets using a lossy neural compression model. \citet{choi2021neural} studied similar neural compression models on plasma data. \citet{huang2023compressing} compress climate data by overfitting a neural network and using the network weights as compressed data representation. \citet{wang2023learning} adopted a classical-neural hybrid approach in medical image compression. Overall, we are unaware of any efforts applying neural compression to astronomical images.

While the astronomical data in public archives \citep{mast2024} are vast, the assembly process for a machine learning suitable corpus requires significant domain knowledge. With dozens of parameters defining each observation (sky region, exposure time, filter, grating, pupil, etc.) and myriad data structures, these archives are too unwieldy for ML practitioners. %We curated datasets by carefully selecting these observational parameters such that trained models would be sufficiently general for practical use, but not so general that model performance suffers.
Previous attempts at ML-friendly corpus creation, such as Galaxy10 \citep{2019MNRAS.483.3255L} (see also \citealt{2023arXiv230711122X} and  \citealt{yolo}), have typically rescaled data to 8-bit RGB images, significantly reducing dynamic range and thereby losing much of the information critical for novel scientific analysis. \citet{2021ApJ...911L..33H} assembled a 266\,GB corpus of processed \texttt{float32} 64$\times$64 pix$^2$ 5-filter image cutouts around bright galaxies. AstroCompress, in contrast, is focused on  \texttt{uint16} images that represent a much wider diversity of real-world raw data, including large regions of low SNR and major imaging artifacts. 

% \paragraph{Lossless compression of scientific data}
% Within astronomy:
% \url{https://docs.google.com/document/d/1ANxdE9RZdysH05trpKqNXp_YhgZ6a6SF9dmT1AM1ECo/edit#heading=h.p43y2y90a8dd}
