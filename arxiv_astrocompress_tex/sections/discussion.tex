\section{Discussion of Future Direction}
\label{sec:discussion}
% We have the following suggestions for future directions:

\textbf{Lossy compression in astronomy} can  likely achieve significantly higher compression ratios compared to lossless methods. For astronomy, where most pixels are dominated by noise, lossy compression is particularly promising. Collaborations between astronomers and machine learning experts could lead to the development of advanced lossy compression algorithms that selectively discard non-essential data, preserving only the scientifically valuable information. One simple approach might mask astronomical sources and prioritize the accuracy of source pixels. Another approach could involve near-lossless compression, which ensures a strict user-defined upper bound on the error of every individual reconstructed pixel \citep{bai2022}. Such an approach would be attractive to astronomers, who may desire a known error measurement for uncertainty propagation. 

%Another innovative approach could involve ``semantic compression,'' where the compression model is designed to incorporate astronomical knowledge. A similar method~\citep{matsubara2022supervised} has already proven effective in retaining essential semantic representation for tasks such as image segmentation and classification. Future investigations may study distortion metrics and rate-distortion curves for lossy methods, which remain beyond the scope for this analysis. 

% \textcolor{red}{One idea might be to losslessly compress source pixels and do lossy compression on background pixels.} A very practical extension of this work would be into near-lossless compression. While traditional image compression codecs focus on imperceptible changes, scientists and other technical practitioners are focused on other properties, such as flux from astrophysical objects.

\textbf{Encode-decode time tradeoffs} factor crucially in remote data transmission. Astronomy prioritizes compression performance, followed by manageable encoding times. Slow decoding on the ground is a non-issue. Traditional compression methods optimize for fast decoding, so future work should explore trading decoding speed for higher compression ratios. This asymmetry favors autoregressive methods that can yield higher compression ratios but slow sequential decoding \citep{yang2023introduction}, with Transformer models being potential candidates (\citet{child2019generating}, \citet{roy2021efficient}).


\textbf{Time-series imaging} has the potential for better compression ratios by exploiting correlations across time. We achieved extremely high compression ratios on the JWST-2D-Res dataset, as the frames were collected back-to-back in time, whereas the SDSS-3DT dataset did not see the same results when compressing multiple frames over time. This may be due to SDSS-3DT images of the same sky location being taken days apart, with atmospheric conditions, moon phase, and other factors introducing too much variance. We encourage more exploration of dataset construction and compression for back-to-back time-series imagery. Such compression will be critical for the operation of the next-generation of wide-field time-domain surveys from space, such as CuRIOS \citep{2022cosp...44.1985G}.

% \textbf{Encode-decode time tradeoffs} are a unique parameter in the remote data transmission challenge. We require encode times to be manageable for space hardware, but we have little restrictions on decode time, which can happen on ground-based supercomputers. We make a key note that traditional compression codecs have optimized heavily for fast decoding, and encourage future work to find ways to trade off decoding speed for increased compression ratios. This asymmetry favors autoregressive methods that typically yield higher compression ratios but slow sequential decoding \citep{mentzer2019practical}.


% \textbf{High diversity, multi-modal datasets} should be trained on, as indicated by the strong performance of models trained on our diverse Keck dataset and evaluated on other datasets. 

\textbf{Further exploration of the data specificity vs. generality spectrum} is needed. There is a wide variety of data types in astronomy, such as spectrometry, radio, and imagery data, and within imagery data, ground-based vs. space-based. Moving down this hierarchy: different telescopes, different instruments within telescopes, and different parameters within instruments follow. As an example, an astronomer setting up an observation for the NIRCAM instrument on JWST may select exposure time, readout pattern, wavelength filter, pupil, etc \citep{jwst2024}. In our study, we purposefully restricted our scope to imagery data of one or a few wavelength filters, but allowed for a wide range of exposure times, readout patterns, and other parameters. We believe this set of data is specialized enough to demonstrate improvements over generic algorithms such as Rice or HCompress, but also accommodates a reasonably wide range of use cases. The right level of specificity will depend on the compression ratios achieved vs. ease of deployment.

We hypothesize that high diversity, multi-modal datasets may be preferable for training compression models, as indicated by strong generalization performance from training on the diverse Keck dataset (see Sec.~\ref{sec:generalization}).
This would make for a multi-purpose compressor that could be a practical successor to JPEG-XL. Eventually, large, multi-modal models and datasets may leverage imaging, spectroscopic and catalogue data all at once. 